{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.181335Z",
     "start_time": "2024-04-30T22:49:44.820803Z"
    }
   },
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "from tabulate import tabulate\n",
    "import time"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph Environment",
   "id": "40bca3c2ebf7d429"
  },
  {
   "cell_type": "code",
   "id": "5a3299232c5dbc09",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.203873Z",
     "start_time": "2024-04-30T22:49:54.184065Z"
    }
   },
   "source": [
    "class GraphEnvironment:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph  # The graph represented as an adjacency list\n",
    "        self.num_nodes = len(graph)\n",
    "        self.global_state = self.initialize_global_state()  # State of every node -> 0 (will be decided), 1 (included), 2 (not included)\n",
    "        self.state = self.initialize_state()    # An adjacency list representing State of every agent as state of the node along with state of all its neighbours\n",
    "\n",
    "    ## Function to initialize self.state as all zeros\n",
    "    def initialize_state(self):\n",
    "        z = [[0]*(len(row)+1) for row in self.graph]\n",
    "        return z\n",
    "\n",
    "    ## Function to initialize self.global_state as all zeros\n",
    "    def initialize_global_state(self):\n",
    "        return np.zeros(self.num_nodes)\n",
    "    \n",
    "    ## Helper function for the remove_adjacent function\n",
    "    def bfs(self, ind, s):\n",
    "        vis = np.zeros(self.num_nodes)\n",
    "        q = []\n",
    "        vis[ind]=1\n",
    "        f = False\n",
    "        q.append(ind)\n",
    "        ## Additional reward which is included as negative reward to punish the agents for choosing adjacent nodes to be included in the set \n",
    "        rew = 1\n",
    "        while len(q):\n",
    "            i = q.pop(0)\n",
    "            for j in self.graph[i]:\n",
    "                if(vis[j]==0 and s[j]==1):\n",
    "                    f = True\n",
    "                    rew += 1\n",
    "                    s[j]=0\n",
    "                    q.append(j)\n",
    "                    \n",
    "        if f:   s[ind] = 0\n",
    "        return rew\n",
    "    \n",
    "    ## Function to set state of every pair of adjacent nodes whose state has been changed to included\n",
    "    def remove_adjacent(self, s):\n",
    "        ## Helper function to help remove all connected nodes which have been set to be included\n",
    "        rew = 0\n",
    "        for i in range(self.num_nodes):\n",
    "            if s[i] == 1 :\n",
    "                rew += self.bfs(i,s)\n",
    "        return s, rew\n",
    "\n",
    "    ## Function to include all nodes in set which are surrounded by all not included nodes as it is always optimal to include this node in set\n",
    "    def include_possible(self, s):\n",
    "        for i in range(self.num_nodes) :\n",
    "            f = True\n",
    "            if s[i] == 0 :\n",
    "                for j in self.graph[i] :\n",
    "                    if s[j] in [0, 1] :\n",
    "                        f = False\n",
    "                        break\n",
    "                if f : s[i] = 1\n",
    "        return s\n",
    "\n",
    "    ## Function to set state of all nodes adjacent to those nodes whose state has been set to be included as not included as taking the nodes which are included they can never be included in the independent set\n",
    "    def exclude_adjacent(self, s):\n",
    "        for i in range(self.num_nodes):\n",
    "            if s[i]==1:\n",
    "                for j in self.graph[i]:\n",
    "                    if s[j]==0:    s[j]=2\n",
    "        return s\n",
    "\n",
    "    ## Function calculates reward of a step in the environment as the change in cardinality of the current independent set, hence including as many nodes as possible gives more reward and trains the agents to do the same\n",
    "    def calculate_reward(self, s1, s2):\n",
    "        sum1=0\n",
    "        sum2=0\n",
    "        for i in range(self.num_nodes):\n",
    "            if s2[i] == 1:\n",
    "                sum2 += 1\n",
    "            if s1[i] == 1:\n",
    "                sum1 += 1\n",
    "        return sum2 - sum1\n",
    "    \n",
    "    ## Returns the termination flag after every step\n",
    "    def check_termination(self, s):\n",
    "        for i in range(self.num_nodes):\n",
    "            if s[i] == 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    ## Function to convert the global state to local state representing the individual states of every agent\n",
    "    def convert_global_local(self, global_s):\n",
    "        local = []\n",
    "        for v in range(self.num_nodes):\n",
    "            temp = [global_s[v]]\n",
    "            for neighbour in self.graph[v]:\n",
    "                temp.append(global_s[neighbour])\n",
    "            local.append(temp)\n",
    "        return local\n",
    "\n",
    "    ## Function to take the step based on action provide in the environment \n",
    "    def step(self, actions):\n",
    "        global_state0 = actions\n",
    "        global_state1, neg_reward = self.remove_adjacent(global_state0)\n",
    "        global_state2 = self.exclude_adjacent(global_state1)\n",
    "        global_state3 = self.include_possible(global_state2)\n",
    "        ## We have given an additional -1 reward for every step to encourage the agent to find the MIS in as few steps as possible\n",
    "        reward = self.calculate_reward(self.global_state, global_state3) - neg_reward - 1\n",
    "        self.global_state = global_state3\n",
    "        self.state = self.convert_global_local(self.global_state)\n",
    "        done = self.check_termination(self.global_state)\n",
    "        return self.state, reward, done\n",
    "\n",
    "    # Reset the environment to the initial state\n",
    "    def reset(self):\n",
    "        self.global_state = self.initialize_global_state()\n",
    "        self.state = self.initialize_state()\n",
    "        return self.state"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing the graph environment ",
   "id": "6d75b7714a5684af"
  },
  {
   "cell_type": "code",
   "id": "334e3c9f6b564d75",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.219533Z",
     "start_time": "2024-04-30T22:49:54.205571Z"
    }
   },
   "source": [
    "graph = np.array([\n",
    "    [1, 2, 5],\n",
    "    [0, 2, 3],\n",
    "    [0, 1, 3, 4],\n",
    "    [1, 2, 4, 5],\n",
    "    [2, 3, 5],\n",
    "    [0, 3, 4]\n",
    "],dtype=object)\n",
    "\n",
    "env = GraphEnvironment(graph)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "dbb4a998243391d6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.228925Z",
     "start_time": "2024-04-30T22:49:54.221542Z"
    }
   },
   "source": [
    "env.reset()\n",
    "s, rew, done = env.step(np.array([2,2,0,2,2,0]))\n",
    "print(\"Reward -> \",rew)\n",
    "print(\"Terminated -> \",done)\n",
    "print(env.state)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward ->  1\n",
      "Terminated ->  True\n",
      "[[2, 2, 1, 1], [2, 2, 1, 2], [1, 2, 2, 2, 2], [2, 2, 1, 2, 1], [2, 1, 2, 1], [1, 2, 2, 2]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PPO Agent class ",
   "id": "d86b27709c8e2225"
  },
  {
   "cell_type": "code",
   "id": "a73487fdd538cbe8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.243860Z",
     "start_time": "2024-04-30T22:49:54.232937Z"
    }
   },
   "source": [
    "## Every PPO Agent contains 2 networks, one is actor which gives the policy according to which action is to be taken and second is critic which predicts the value of being in a state\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 0.2  # PPO clip ratio\n",
    "        self.lr_actor = 0.001  # Actor learning rate\n",
    "        self.lr_critic = 0.001  # Critic learning rate\n",
    "\n",
    "        # Build actor and critic networks\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        # Optimizers\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr_actor)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr_critic)\n",
    "\n",
    "    ## Neural Network model builder for the actor model containing a single hidden layer with 64 nodes\n",
    "    def build_actor(self):\n",
    "        model = models.Sequential([\n",
    "            layers.InputLayer(input_shape=(self.state_dim,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(self.action_dim, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    ## Neural Network model builder for the critic model containing a single hidden layer with 64 nodes\n",
    "    def build_critic(self):\n",
    "        model = models.Sequential([\n",
    "            layers.InputLayer(input_shape=(self.state_dim,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    ## Function to sample actions for the agent based on policy from the actor\n",
    "    def select_action(self, state):\n",
    "        action_probs = self.actor(tf.convert_to_tensor(np.expand_dims(state, axis=0)))[0]\n",
    "        sample = tf.random.uniform(shape=(1,), minval=0, maxval=1)[0] \n",
    "        cum_prob = 0\n",
    "        for i, p in enumerate(action_probs):\n",
    "            if sample <= cum_prob + p:\n",
    "                return i\n",
    "            cum_prob += p\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "dbc1225ce835a412",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.251939Z",
     "start_time": "2024-04-30T22:49:54.245870Z"
    }
   },
   "source": [
    "## Function to return discounted rewards for a episode\n",
    "def discounted_rewards(rewards, gamma=0.99):\n",
    "        r = np.array(rewards)\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(len(r))):\n",
    "            running_add = running_add * gamma + r[t]\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the Multi Agent Model",
   "id": "eea25082b76572c6"
  },
  {
   "cell_type": "code",
   "source": [
    "def train_multi_agent(env, agents, num_episodes, epochs = 10, clip_epsilon = 0.2):\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        ## Getting an episode/trajectory\n",
    "        state = env.reset()\n",
    "        all_states, all_actions, all_rewards = [], [], []\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while (not done) and steps<50:\n",
    "            steps += 1\n",
    "            actions = []\n",
    "            all_states.append(state)\n",
    "            for agent_id, agent in enumerate(agents):\n",
    "                if state[agent_id][0] !=0:\n",
    "                    action = state[agent_id][0]\n",
    "                else:\n",
    "                    action = agent.select_action(state[agent_id])\n",
    "                actions.append(action)\n",
    "\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            state = next_states\n",
    "            all_rewards.append(reward)\n",
    "            all_actions.append(actions)\n",
    "\n",
    "        all_rewards = tf.convert_to_tensor(all_rewards, dtype=tf.float32)\n",
    "        \n",
    "        ## Optimizing agent parameters\n",
    "        for agent_id, agent in enumerate(agents):\n",
    "            with tf.GradientTape() as tape_critic:\n",
    "                ## Probabilities according to policy based on which trajectory is chosen\n",
    "                probs_old = tf.stack([tf.gather(agent.actor(tf.convert_to_tensor(np.expand_dims(st[agent_id], axis=0)))[0], at[agent_id]) for st, at in zip(all_states, all_actions)])\n",
    "                \n",
    "                ## Used the agent critic model to predict the value function for the states in the trajectory\n",
    "                values = tf.convert_to_tensor([agent.critic(tf.convert_to_tensor(np.expand_dims(st[agent_id], axis=0)))[0, 0] for st in all_states])\n",
    "                disc_rewards = tf.convert_to_tensor(discounted_rewards(all_rewards))\n",
    "                advantages = disc_rewards - values  ## Getting Advantages array\n",
    "                combined_loss = 0   ## Overall loss used to optimize critic parameters\n",
    "                for _ in range(epochs):\n",
    "                    with tf.GradientTape() as tape_actor:\n",
    "                        ## Updated Policy\n",
    "                        probs_new = tf.stack([tf.gather(agent.actor(tf.convert_to_tensor(np.expand_dims(st[agent_id], axis=0)))[0], at[agent_id]) for st, at in zip(all_states, all_actions)])\n",
    "                        ratio = probs_new / probs_old   ## Policy ratio\n",
    "                        clipped_ratio = tf.clip_by_value(ratio, 1 - clip_epsilon, 1 + clip_epsilon) ## Clipped policy ratio\n",
    "                        policy_loss = tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))\n",
    "                        value_loss = tf.reduce_mean(tf.keras.losses.MSE(disc_rewards, values))\n",
    "                        total_loss = policy_loss + value_loss\n",
    "                        combined_loss += total_loss\n",
    "                    ## Calculating gradients and descending gradients\n",
    "                    grads_actor = tape_actor.gradient(total_loss, agent.actor.trainable_weights)\n",
    "                    agent.actor_optimizer.apply_gradients(zip(grads_actor, agent.actor.trainable_weights))\n",
    "            grads_critic = tape_critic.gradient(combined_loss, agent.critic.trainable_weights)\n",
    "            agent.critic_optimizer.apply_gradients(zip(grads_critic, agent.critic.trainable_weights))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.269531Z",
     "start_time": "2024-04-30T22:49:54.254955Z"
    }
   },
   "id": "1e2bf223ba260a21",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "c1399cf1505e515d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.276216Z",
     "start_time": "2024-04-30T22:49:54.271541Z"
    }
   },
   "source": [
    "# Code block to test training on custom graph\n",
    "# graph = np.array([\n",
    "#     [1, 2, 5],\n",
    "#     [0, 2, 3],\n",
    "#     [0, 1, 3, 4],\n",
    "#     [1, 2, 4, 5],\n",
    "#     [2, 3, 5],\n",
    "#     [0, 3, 4]\n",
    "# ],dtype=object)\n",
    "# \n",
    "# env = GraphEnvironment(graph)\n",
    "# \n",
    "# agents = [PPOAgent(len(v)+1, 3) for v in env.graph]\n",
    "# num_episodes = 100\n",
    "# \n",
    "# train_multi_agent(env, agents, num_episodes, epochs=5)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "5101e4245f9e82a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.285794Z",
     "start_time": "2024-04-30T22:49:54.278238Z"
    }
   },
   "source": [
    "## Function to use the agents trained during training to sample multiple solutions to return the best among them as the prediction of the reinforcement learning algorithm\n",
    "def find_MIS(env, agents, num_solutions=20):\n",
    "    mis = 0\n",
    "    for episode in range(num_solutions):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while (not done) and steps<50:\n",
    "            steps += 1\n",
    "            actions = []\n",
    "            for agent_id, agent in enumerate(agents):\n",
    "                if state[agent_id][0] !=0:\n",
    "                    action = state[agent_id][0]\n",
    "                else:\n",
    "                    action = agent.select_action(state[agent_id])\n",
    "                actions.append(action)\n",
    "    \n",
    "            state, reward, done = env.step(actions)\n",
    "        mis = max(mis,env.global_state.count(1))\n",
    "    return mis"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing the algorithm",
   "id": "3ccffe105c58e86c"
  },
  {
   "cell_type": "code",
   "source": [
    "## Function to combine all the steps in Multi agent RL algorithm\n",
    "def MARL_MIS_algorithm(G, num_episodes=100, epochs=10, num_solutions=20):\n",
    "    env = GraphEnvironment(G)\n",
    "    agents = [PPOAgent(len(v)+1, 3) for v in env.graph]\n",
    "    train_multi_agent(env, agents, num_episodes, epochs=epochs)\n",
    "    return find_MIS(env, agents, num_solutions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.296123Z",
     "start_time": "2024-04-30T22:49:54.287823Z"
    }
   },
   "id": "bd3c12b3d359bae5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "## Function to generate random graphs with the help of NetworkX library\n",
    "def generate_random_graph(num_nodes, edge_prob, seed):\n",
    "    g = nx.gnp_random_graph(num_nodes, edge_prob, seed=  seed)\n",
    "    adj_list = [list(g.neighbors(node)) for node in g.nodes()]\n",
    "    return adj_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.303493Z",
     "start_time": "2024-04-30T22:49:54.297133Z"
    }
   },
   "id": "c1347dca4232a0f8",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function to return greedy MIS ",
   "id": "7a9f65a3ac1166c9"
  },
  {
   "cell_type": "code",
   "source": [
    "def greedy_solution(G):\n",
    "    independent_set = []\n",
    "    remaining_vertices = set(range(len(G)))\n",
    "\n",
    "    while remaining_vertices:\n",
    "        v = remaining_vertices.pop()\n",
    "        independent_set.append(v)\n",
    "        neighbors_to_remove = set(G[v])\n",
    "        remaining_vertices.difference_update(neighbors_to_remove)\n",
    "        remaining_vertices.discard(v) \n",
    "\n",
    "    return len(independent_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.312984Z",
     "start_time": "2024-04-30T22:49:54.305503Z"
    }
   },
   "id": "26703798d924c805",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function to return exact MIS using brute force",
   "id": "f6750882d671ae90"
  },
  {
   "cell_type": "code",
   "source": [
    "def is_independent_set(vertices, adj_list):\n",
    "    for i in vertices:\n",
    "        for j in vertices:\n",
    "            if i != j and j in adj_list[i]:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def find_maximum_independent_set(current_set, index, adj_list, best_set):\n",
    "    if index == len(adj_list):\n",
    "        if is_independent_set(current_set, adj_list) and len(current_set) > len(best_set[0]):\n",
    "            best_set[0] = current_set.copy()\n",
    "        return\n",
    "    \n",
    "    current_set.add(index)\n",
    "    find_maximum_independent_set(current_set, index + 1, adj_list, best_set)\n",
    "    current_set.remove(index)\n",
    "    find_maximum_independent_set(current_set, index + 1, adj_list, best_set)\n",
    "\n",
    "def exact_solution(adj_list):\n",
    "    best_set = [set()]\n",
    "    find_maximum_independent_set(set(), 0, adj_list, best_set)\n",
    "    return len(best_set[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.323468Z",
     "start_time": "2024-04-30T22:49:54.314993Z"
    }
   },
   "id": "bc96f3ee41653516",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "def testing(num_graphs=10, num_nodes=10, edge_prob=0.5, num_episodes=100, epochs=15, num_solutions=20):\n",
    "    data = []\n",
    "    for i in tqdm(range(num_graphs)):\n",
    "        G = generate_random_graph(num_nodes,edge_prob,i)\n",
    "        greedy_mis = greedy_solution(G)\n",
    "        MARL_mis = MARL_MIS_algorithm(G, num_episodes=num_episodes, epochs=epochs, num_solutions= num_solutions)\n",
    "        exact_mis = exact_solution(G)\n",
    "        data.append((greedy_mis,MARL_mis,exact_mis))\n",
    "        \n",
    "    headers = [\"Greedy MIS\", \"MARL MIS\", \"Exact MIS\"]\n",
    "    print(tabulate(data, headers=headers, tablefmt='grid'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T22:49:54.337003Z",
     "start_time": "2024-04-30T22:49:54.328575Z"
    }
   },
   "id": "2f1a44de1ccc87fe",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "testing(num_graphs=5, num_nodes=10, edge_prob=0.5, num_episodes=50, epochs=10, num_solutions=5)",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2024-05-01T00:19:01.192357Z",
     "start_time": "2024-04-30T23:54:50.802848Z"
    }
   },
   "id": "e8fff509364c0f97",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [24:10<00:00, 290.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+-------------+\n",
      "|   Greedy MIS |   MARL MIS |   Exact MIS |\n",
      "+==============+============+=============+\n",
      "|            4 |          3 |           4 |\n",
      "+--------------+------------+-------------+\n",
      "|            3 |          3 |           3 |\n",
      "+--------------+------------+-------------+\n",
      "|            3 |          4 |           4 |\n",
      "+--------------+------------+-------------+\n",
      "|            4 |          4 |           4 |\n",
      "+--------------+------------+-------------+\n",
      "|            3 |          4 |           5 |\n",
      "+--------------+------------+-------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T23:21:11.561741Z",
     "start_time": "2024-04-30T23:21:11.557724Z"
    }
   },
   "id": "5342f37f09c71de0",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
